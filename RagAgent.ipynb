{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f76608",
   "metadata": {},
   "source": [
    "# RAG Agent: Intelligent Document Q&A System\n",
    "\n",
    "A production-ready Retrieval-Augmented Generation (RAG) system powered by NVIDIA AI endpoints, LangChain, and FAISS vector store.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a complete RAG pipeline that:\n",
    "- Loads and processes research papers from arXiv\n",
    "- Creates semantic embeddings using NVIDIA's embedding models\n",
    "- Stores documents in a FAISS vector database\n",
    "- Retrieves relevant context for user queries\n",
    "- Generates grounded, citation-backed responses using LLMs\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Intelligent Retrieval**: Uses semantic search to find the most relevant document chunks\n",
    "- **Context Reordering**: Applies long-context reordering to optimize retrieval quality\n",
    "- **Grounded Generation**: Responses are strictly based on retrieved documents\n",
    "- **Streaming Output**: Real-time response generation for better UX\n",
    "- **Production Ready**: Modular design for easy deployment and integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0b3ed",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Install required dependencies and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cbd3e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "âœ“ Environment setup complete\n",
      "âœ“ Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "# Set NVIDIA API Key\n",
    "import os\n",
    "os.environ['NVIDIA_API_KEY'] = 'YOUR_NVIDIA_API_KEY'\n",
    "\n",
    "# Install required packages\n",
    "%pip install -q langchain langchain-nvidia-ai-endpoints gradio\n",
    "%pip install -q arxiv pymupdf faiss-cpu\n",
    "\n",
    "# Import core libraries\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from operator import itemgetter\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "print(\"âœ“ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16b8736",
   "metadata": {},
   "source": [
    "## 2. Initialize AI Models\n",
    "\n",
    "Configure NVIDIA AI endpoints for embeddings and language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3bab222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ AI models initialized successfully\n",
      "  - Embedding Model: nvidia/nv-embed-v1\n",
      "  - LLM: meta/llama-3.1-70b-instruct\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model for semantic search\n",
    "embedder = NVIDIAEmbeddings(\n",
    "    model=\"nvidia/nv-embed-v1\",\n",
    "    truncate=\"END\"\n",
    ")\n",
    "\n",
    "# Initialize LLM for response generation\n",
    "# âš ï¸ IMPORTANT: Use only valid NVIDIA-hosted models:\n",
    "# - \"meta/llama-3.1-8b-instruct\" (Fast, 8B parameters)\n",
    "# - \"meta/llama-3.1-70b-instruct\" (Powerful, 70B parameters)\n",
    "# - \"nvidia/llama-3.1-nemotron-70b-instruct\" (NVIDIA's fine-tuned version)\n",
    "# - \"mistralai/mixtral-8x7b-instruct-v0.1\" (Mistral's MoE model)\n",
    "\n",
    "instruct_llm = ChatNVIDIA(model=\"openai/gpt-oss-120b\")\n",
    "llm = instruct_llm | StrOutputParser()\n",
    "\n",
    "print(\"âœ“ AI models initialized successfully\")\n",
    "print(f\"  - Embedding Model: nvidia/nv-embed-v1\")\n",
    "print(f\"  - LLM: meta/llama-3.1-70b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7dd6e",
   "metadata": {},
   "source": [
    "## 3. Load Documents & Create Vector Store\n",
    "\n",
    "Load research papers from arXiv and create FAISS embeddings on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8a69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading papers from arXiv...\n",
      "  âœ“ Loaded: Retrieval-Augmented Generation for Knowledge-Intensive NLP T...\n",
      "  âœ“ Loaded: Retrieval-Augmented Generation for Knowledge-Intensive NLP T...\n",
      "  âœ“ Loaded: Attention Is All You Need...\n",
      "  âœ“ Loaded: Attention Is All You Need...\n",
      "  âœ“ Loaded: Visual Instruction Tuning...\n",
      "  âœ“ Loaded: Visual Instruction Tuning...\n",
      "  âœ“ Loaded: Llama 2: Open Foundation and Fine-Tuned Chat Models...\n",
      "\n",
      "âœ“ Total papers loaded: 4\n",
      "\n",
      "Splitting documents into chunks...\n",
      "âœ“ Created 577 document chunks\n",
      "\n",
      "Creating embeddings and building vector store...\n",
      "  âœ“ Loaded: Llama 2: Open Foundation and Fine-Tuned Chat Models...\n",
      "\n",
      "âœ“ Total papers loaded: 4\n",
      "\n",
      "Splitting documents into chunks...\n",
      "âœ“ Created 577 document chunks\n",
      "\n",
      "Creating embeddings and building vector store...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Define arXiv paper IDs to load (you can customize this list)\n",
    "arxiv_ids = [\n",
    "    \"2005.11401\",  # RAG: Retrieval-Augmented Generation\n",
    "    \"1706.03762\",  # Attention Is All You Need (Transformers)\n",
    "    \"2304.08485\",  # GPT-4 Technical Report\n",
    "    \"2307.09288\",  # Llama 2 Paper\n",
    "]\n",
    "\n",
    "print(\"Loading papers from arXiv...\")\n",
    "all_docs = []\n",
    "\n",
    "for arxiv_id in arxiv_ids:\n",
    "    try:\n",
    "        loader = ArxivLoader(query=arxiv_id, load_max_docs=1)\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "        print(f\"  âœ“ Loaded: {docs[0].metadata.get('Title', arxiv_id)[:60]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Failed to load {arxiv_id}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nâœ“ Total papers loaded: {len(all_docs)}\")\n",
    "\n",
    "# Split documents into chunks for better retrieval\n",
    "print(\"\\nSplitting documents into chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(all_docs)\n",
    "print(f\"âœ“ Created {len(chunks)} document chunks\")\n",
    "\n",
    "# Create FAISS vector store from documents\n",
    "print(\"\\nCreating embeddings and building vector store...\")\n",
    "docstore = FAISS.from_documents(chunks, embedder)\n",
    "print(f\"âœ“ Vector store created successfully\")\n",
    "print(f\"  - Total vectors: {len(chunks)}\")\n",
    "print(f\"  - Sample chunk: {chunks[0].page_content[:100]}...\")\n",
    "\n",
    "# Save the vector store locally\n",
    "print(\"\\nSaving vector store to disk...\")\n",
    "docstore.save_local(\"docstore_index\")\n",
    "print(\"âœ“ Vector store saved to 'docstore_index' folder\")\n",
    "\n",
    "# Create a zip file of the vector store\n",
    "print(\"\\nCreating zip archive...\")\n",
    "shutil.make_archive(\"docstore_index\", 'zip', \"docstore_index\")\n",
    "print(\"âœ“ Zip file created: 'docstore_index.zip'\")\n",
    "print(\"\\nðŸ“¦ You can download 'docstore_index.zip' and reuse it next time!\")\n",
    "print(\"   This saves time by avoiding re-downloading and re-embedding papers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850f415",
   "metadata": {},
   "source": [
    "## 3B. Load Existing Vector Store (Alternative)\n",
    "\n",
    "**Use this cell if you already have a saved vector store zip file.**  \n",
    "Skip this if you just created a new vector store in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47519ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Path to your saved vector store zip file\n",
    "zip_file_path = \"docstore_index.zip\"\n",
    "\n",
    "# Check if the zip file exists\n",
    "if os.path.exists(zip_file_path):\n",
    "    print(f\"ðŸ“¦ Found existing vector store: {zip_file_path}\")\n",
    "    \n",
    "    # Extract the zip file\n",
    "    print(\"Extracting vector store...\")\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "    print(\"âœ“ Vector store extracted successfully\")\n",
    "    \n",
    "    # Load the vector store\n",
    "    print(\"\\nLoading vector store from disk...\")\n",
    "    docstore = FAISS.load_local(\n",
    "        \"docstore_index\",\n",
    "        embedder,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    \n",
    "    print(\"âœ“ Vector store loaded successfully\")\n",
    "    print(f\"  - Total vectors: {docstore.index.ntotal}\")\n",
    "    print(\"\\nðŸš€ Ready to use! You can now proceed to build the RAG pipeline.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ Error: Vector store file '{zip_file_path}' not found!\")\n",
    "    print(\"Please run the cell above to create a new vector store, or\")\n",
    "    print(\"upload your existing 'docstore_index.zip' file to this directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65471a27",
   "metadata": {},
   "source": [
    "## 4. Build RAG Pipeline\n",
    "\n",
    "Create the complete RAG chain with retrieval and generation components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "079e7f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Prompt templates configured\n"
     ]
    }
   ],
   "source": [
    "# Utility: Format retrieved documents into readable context\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Convert document list to formatted string with citations.\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "\n",
    "# Define the chat prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a helpful document chatbot. Answer questions based solely on the provided context.\"\n",
    "    \" User question: {input}\\n\\n\"\n",
    "    \"Retrieved Context:\\n{context}\\n\\n\"\n",
    "    \"Instructions:\\n\"\n",
    "    \"- Only use information from the retrieved context\\n\"\n",
    "    \"- Cite sources when making claims\\n\"\n",
    "    \"- Be conversational and clear\\n\"\n",
    "    \"- If context is insufficient, acknowledge limitations\\n\\n\"\n",
    "    \"Question: {input}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Utility: Stream output from chain results\n",
    "def output_puller(inputs):\n",
    "    \"\"\"Extract and yield the 'output' field from runnable results.\"\"\"\n",
    "    if isinstance(inputs, dict):\n",
    "        inputs = [inputs]\n",
    "    for token in inputs:\n",
    "        if token.get('output'):\n",
    "            yield token.get('output')\n",
    "\n",
    "print(\"âœ“ Prompt templates configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34506afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Retrieval chain built\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Build the retrieval chain\n",
    "# ====================================\n",
    "\n",
    "# Initialize long-context reordering for better retrieval quality\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "\n",
    "# Create retriever from vector store (top-5 most relevant chunks)\n",
    "doc_retriever = docstore.as_retriever(search_kwargs={'k': 5})\n",
    "\n",
    "\n",
    "def _context_to_text(docs):\n",
    "    \"\"\"Convert retrieved documents to formatted text string.\"\"\"\n",
    "    if not docs:\n",
    "        return (\n",
    "            \"No relevant passages found in the knowledge base. \"\n",
    "            \"Please rephrase your question or ask about a different topic.\"\n",
    "        )\n",
    "    return docs2str(docs)\n",
    "\n",
    "\n",
    "# Build context retrieval pipeline\n",
    "context_getter = (\n",
    "    itemgetter('input')\n",
    "    | doc_retriever\n",
    "    | long_reorder\n",
    "    | RunnableLambda(_context_to_text)\n",
    ")\n",
    "\n",
    "# Complete retrieval chain: input -> {input, context}\n",
    "retrieval_chain = (\n",
    "    {'input': (lambda x: x)}\n",
    "    | RunnableAssign({'context': context_getter})\n",
    ")\n",
    "\n",
    "print(\"âœ“ Retrieval chain built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "373c4e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Generation chain built\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build the generation chain\n",
    "# ====================================\n",
    "\n",
    "# Create response generation pipeline\n",
    "response_chain = (\n",
    "    {\n",
    "        'input': itemgetter('input'),\n",
    "        'context': itemgetter('context'),\n",
    "    }\n",
    "    | chat_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# Wrap output for streaming compatibility\n",
    "generator_chain = {'output': response_chain} | RunnableLambda(output_puller)\n",
    "\n",
    "print(\"âœ“ Generation chain built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b39abbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Complete RAG pipeline assembled\n",
      "\n",
      "Pipeline Architecture:\n",
      "  1. User Query â†’ Embedding\n",
      "  2. Semantic Search â†’ Top-K Documents\n",
      "  3. Context Reordering â†’ Optimized Context\n",
      "  4. LLM Generation â†’ Grounded Response\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Combine into complete RAG pipeline\n",
    "# =============================================\n",
    "\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "print(\"âœ“ Complete RAG pipeline assembled\")\n",
    "print(\"\\nPipeline Architecture:\")\n",
    "print(\"  1. User Query â†’ Embedding\")\n",
    "print(\"  2. Semantic Search â†’ Top-K Documents\")\n",
    "print(\"  3. Context Reordering â†’ Optimized Context\")\n",
    "print(\"  4. LLM Generation â†’ Grounded Response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c78c8",
   "metadata": {},
   "source": [
    "## 5. Test the RAG Agent\n",
    "\n",
    "Run sample queries to validate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df6bdf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUERY: What are the latest developments in large language models?\n",
      "======================================================================\n",
      "According to the retrieved context, the latest developments in large language models (LLMs) have been remarkable, with several new models being proposed and released. Some notable developments include the rise of open-source models such as BLOOM (Scao et al., 2022), OPT (Zhang et al., 2022), and Falcon (Penedo et al., 2023), which have been shown to match the performance of closed-source competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022).\n",
      "\n",
      "Additionally, there has been a focus on fine-tuning LLMs for specific tasks, such as dialogue use cases. For example, the Llama 2 model, developed by Meta, has been optimized for chat interfaces and has shown promising results in terms of helpfulness and safety (Touvron et al., 2023).\n",
      "\n",
      "Another trend is the increasing use of large vision models, which are being used to solve tasks independently, with language playing a supporting role in mapping visual signals to language semantics (Taylor et al., 2022).\n",
      "\n",
      "However, it's worth noting that the training methodology for LLMs is still relatively straightforward, but high computational requirements have limited the development of LLMs to a few players. Furthermore, fine-tuning LLMs for human preferences and safety still requires significant costs in compute and human annotation.\n",
      "\n",
      "Overall, the field of LLMs is rapidly evolving, and new developments are being made regularly. But, it's also important to acknowledge that there are still limitations and challenges to be addressed in the community.\n",
      "======================================================================\n",
      "According to the retrieved context, the latest developments in large language models (LLMs) have been remarkable, with several new models being proposed and released. Some notable developments include the rise of open-source models such as BLOOM (Scao et al., 2022), OPT (Zhang et al., 2022), and Falcon (Penedo et al., 2023), which have been shown to match the performance of closed-source competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022).\n",
      "\n",
      "Additionally, there has been a focus on fine-tuning LLMs for specific tasks, such as dialogue use cases. For example, the Llama 2 model, developed by Meta, has been optimized for chat interfaces and has shown promising results in terms of helpfulness and safety (Touvron et al., 2023).\n",
      "\n",
      "Another trend is the increasing use of large vision models, which are being used to solve tasks independently, with language playing a supporting role in mapping visual signals to language semantics (Taylor et al., 2022).\n",
      "\n",
      "However, it's worth noting that the training methodology for LLMs is still relatively straightforward, but high computational requirements have limited the development of LLMs to a few players. Furthermore, fine-tuning LLMs for human preferences and safety still requires significant costs in compute and human annotation.\n",
      "\n",
      "Overall, the field of LLMs is rapidly evolving, and new developments are being made regularly. But, it's also important to acknowledge that there are still limitations and challenges to be addressed in the community.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic query with streaming output\n",
    "print(\"=\" * 70)\n",
    "print(\"QUERY: What are the latest developments in large language models?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for token in rag_chain.stream(\"What are the latest developments in large language models?\"):\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f10f4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "QUERY: what is the latest knowlege you have about disadvantagers of RAG\n",
      "======================================================================\n",
      "Based on the provided context, the potential downsides or disadvantages of RAG (Retrieval-Augmented Generation) models are:\n",
      "\n",
      "1. **Misinformation and bias**: Wikipedia, or any external knowledge source, may contain errors or biases, which can be perpetuated by RAG models [14].\n",
      "2. **Misuse for malicious purposes**: RAG models can be used to generate abusive, fake, or misleading content, impersonate others, or automate spam/phishing content, similar to concerns raised about GPT-2 [50].\n",
      "3. **Job automation**: Advanced language models like RAG may lead to the automation of various jobs in the coming decades [16].\n",
      "4. **Hallucinations**: Although RAG is more strongly grounded in factual knowledge than previous models, it can still \"hallucinate\" and generate responses that are not entirely factual.\n",
      "\n",
      "These potential downsides are mentioned in the context as potential risks associated with RAG models, and the authors suggest employing AI systems to mitigate these risks by fighting against misleading content and automated spam/phishing.\n",
      "======================================================================\n",
      "Based on the provided context, the potential downsides or disadvantages of RAG (Retrieval-Augmented Generation) models are:\n",
      "\n",
      "1. **Misinformation and bias**: Wikipedia, or any external knowledge source, may contain errors or biases, which can be perpetuated by RAG models [14].\n",
      "2. **Misuse for malicious purposes**: RAG models can be used to generate abusive, fake, or misleading content, impersonate others, or automate spam/phishing content, similar to concerns raised about GPT-2 [50].\n",
      "3. **Job automation**: Advanced language models like RAG may lead to the automation of various jobs in the coming decades [16].\n",
      "4. **Hallucinations**: Although RAG is more strongly grounded in factual knowledge than previous models, it can still \"hallucinate\" and generate responses that are not entirely factual.\n",
      "\n",
      "These potential downsides are mentioned in the context as potential risks associated with RAG models, and the authors suggest employing AI systems to mitigate these risks by fighting against misleading content and automated spam/phishing.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Custom query\n",
    "query = input(\"Enter your question: \")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"QUERY: {query}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for token in rag_chain.stream(query):\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc9f167",
   "metadata": {},
   "source": [
    "## 6. Interactive Chat Interface (Optional)\n",
    "\n",
    "Launch a Gradio web interface for easier interaction with the RAG agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff1895a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anmol\\Desktop\\sem 5\\projects\\RAGagent\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def chat_interface(message, history):\n",
    "    \"\"\"Process user message and return RAG response.\"\"\"\n",
    "    response = \"\"\n",
    "    for token in rag_chain.stream(message):\n",
    "        response += token\n",
    "    return response\n",
    "\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_interface,\n",
    "    title=\"ðŸ¤– RAG Agent: Intelligent Document Q&A\",\n",
    "    description=\"Ask questions about research papers in the knowledge base. Responses are grounded in retrieved documents.\",\n",
    "    examples=[\n",
    "        \"What are the key findings in recent NLP research?\",\n",
    "        \"Explain the concept of retrieval-augmented generation\",\n",
    "        \"What improvements have been made to transformer models?\",\n",
    "    ],\n",
    "    theme=gr.themes.Soft(),\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
