{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f76608",
   "metadata": {},
   "source": [
    "# RAG Agent: Intelligent Document Q&A System\n",
    "\n",
    "A production-ready Retrieval-Augmented Generation (RAG) system powered by NVIDIA AI endpoints, LangChain, and FAISS vector store.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a complete RAG pipeline that:\n",
    "- Loads and processes research papers from arXiv\n",
    "- Creates semantic embeddings using NVIDIA's embedding models\n",
    "- Stores documents in a FAISS vector database\n",
    "- Retrieves relevant context for user queries\n",
    "- Generates grounded, citation-backed responses using LLMs\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Intelligent Retrieval**: Uses semantic search to find the most relevant document chunks\n",
    "- **Context Reordering**: Applies long-context reordering to optimize retrieval quality\n",
    "- **Grounded Generation**: Responses are strictly based on retrieved documents\n",
    "- **Streaming Output**: Real-time response generation for better UX\n",
    "- **Production Ready**: Modular design for easy deployment and integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0b3ed",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Install required dependencies and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q langchain langchain-nvidia-ai-endpoints gradio\n",
    "%pip install -q arxiv pymupdf faiss-cpu\n",
    "\n",
    "# Import core libraries\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from operator import itemgetter\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "print(\"âœ“ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16b8736",
   "metadata": {},
   "source": [
    "## 2. Initialize AI Models\n",
    "\n",
    "Configure NVIDIA AI endpoints for embeddings and language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bab222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model for semantic search\n",
    "embedder = NVIDIAEmbeddings(\n",
    "    model=\"nvidia/nv-embed-v1\",\n",
    "    truncate=\"END\"\n",
    ")\n",
    "\n",
    "# Initialize LLM for response generation\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "llm = instruct_llm | StrOutputParser()\n",
    "\n",
    "print(\"âœ“ AI models initialized successfully\")\n",
    "print(f\"  - Embedding Model: nvidia/nv-embed-v1\")\n",
    "print(f\"  - LLM: meta/llama-3.1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7dd6e",
   "metadata": {},
   "source": [
    "## 3. Load Document Store\n",
    "\n",
    "Load pre-built FAISS vector store containing embedded research papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and load the FAISS index\n",
    "!tar xzvf docstore_index.tgz\n",
    "\n",
    "# Load the vector store with document embeddings\n",
    "docstore = FAISS.load_local(\n",
    "    \"docstore_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Get all documents from the store\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "print(f\"âœ“ Document store loaded successfully\")\n",
    "print(f\"  - Total documents: {len(docstore.docstore._dict)}\")\n",
    "print(f\"  - Sample paper: {docs[0].metadata.get('Title', 'Unknown')[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65471a27",
   "metadata": {},
   "source": [
    "## 4. Build RAG Pipeline\n",
    "\n",
    "Create the complete RAG chain with retrieval and generation components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e7f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Format retrieved documents into readable context\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Convert document list to formatted string with citations.\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "\n",
    "# Define the chat prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a helpful document chatbot. Answer questions based solely on the provided context.\"\n",
    "    \" User question: {input}\\n\\n\"\n",
    "    \"Retrieved Context:\\n{context}\\n\\n\"\n",
    "    \"Instructions:\\n\"\n",
    "    \"- Only use information from the retrieved context\\n\"\n",
    "    \"- Cite sources when making claims\\n\"\n",
    "    \"- Be conversational and clear\\n\"\n",
    "    \"- If context is insufficient, acknowledge limitations\\n\\n\"\n",
    "    \"Question: {input}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Utility: Stream output from chain results\n",
    "def output_puller(inputs):\n",
    "    \"\"\"Extract and yield the 'output' field from runnable results.\"\"\"\n",
    "    if isinstance(inputs, dict):\n",
    "        inputs = [inputs]\n",
    "    for token in inputs:\n",
    "        if token.get('output'):\n",
    "            yield token.get('output')\n",
    "\n",
    "print(\"âœ“ Prompt templates configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34506afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Build the retrieval chain\n",
    "# ====================================\n",
    "\n",
    "# Initialize long-context reordering for better retrieval quality\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "\n",
    "# Create retriever from vector store (top-5 most relevant chunks)\n",
    "doc_retriever = docstore.as_retriever(search_kwargs={'k': 5})\n",
    "\n",
    "\n",
    "def _context_to_text(docs):\n",
    "    \"\"\"Convert retrieved documents to formatted text string.\"\"\"\n",
    "    if not docs:\n",
    "        return (\n",
    "            \"No relevant passages found in the knowledge base. \"\n",
    "            \"Please rephrase your question or ask about a different topic.\"\n",
    "        )\n",
    "    return docs2str(docs)\n",
    "\n",
    "\n",
    "# Build context retrieval pipeline\n",
    "context_getter = (\n",
    "    itemgetter('input')\n",
    "    | doc_retriever\n",
    "    | long_reorder\n",
    "    | RunnableLambda(_context_to_text)\n",
    ")\n",
    "\n",
    "# Complete retrieval chain: input -> {input, context}\n",
    "retrieval_chain = (\n",
    "    {'input': (lambda x: x)}\n",
    "    | RunnableAssign({'context': context_getter})\n",
    ")\n",
    "\n",
    "print(\"âœ“ Retrieval chain built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c4e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build the generation chain\n",
    "# ====================================\n",
    "\n",
    "# Create response generation pipeline\n",
    "response_chain = (\n",
    "    {\n",
    "        'input': itemgetter('input'),\n",
    "        'context': itemgetter('context'),\n",
    "    }\n",
    "    | chat_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# Wrap output for streaming compatibility\n",
    "generator_chain = {'output': response_chain} | RunnableLambda(output_puller)\n",
    "\n",
    "print(\"âœ“ Generation chain built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Combine into complete RAG pipeline\n",
    "# =============================================\n",
    "\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "print(\"âœ“ Complete RAG pipeline assembled\")\n",
    "print(\"\\nPipeline Architecture:\")\n",
    "print(\"  1. User Query â†’ Embedding\")\n",
    "print(\"  2. Semantic Search â†’ Top-K Documents\")\n",
    "print(\"  3. Context Reordering â†’ Optimized Context\")\n",
    "print(\"  4. LLM Generation â†’ Grounded Response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c78c8",
   "metadata": {},
   "source": [
    "## 5. Test the RAG Agent\n",
    "\n",
    "Run sample queries to validate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6bdf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic query with streaming output\n",
    "print(\"=\" * 70)\n",
    "print(\"QUERY: What are the latest developments in large language models?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for token in rag_chain.stream(\"What are the latest developments in large language models?\"):\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f10f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Custom query\n",
    "query = input(\"Enter your question: \")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"QUERY: {query}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for token in rag_chain.stream(query):\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc9f167",
   "metadata": {},
   "source": [
    "## 6. Interactive Chat Interface (Optional)\n",
    "\n",
    "Launch a Gradio web interface for easier interaction with the RAG agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1895a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def chat_interface(message, history):\n",
    "    \"\"\"Process user message and return RAG response.\"\"\"\n",
    "    response = \"\"\n",
    "    for token in rag_chain.stream(message):\n",
    "        response += token\n",
    "    return response\n",
    "\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_interface,\n",
    "    title=\"ðŸ¤– RAG Agent: Intelligent Document Q&A\",\n",
    "    description=\"Ask questions about research papers in the knowledge base. Responses are grounded in retrieved documents.\",\n",
    "    examples=[\n",
    "        \"What are the key findings in recent NLP research?\",\n",
    "        \"Explain the concept of retrieval-augmented generation\",\n",
    "        \"What improvements have been made to transformer models?\",\n",
    "    ],\n",
    "    theme=gr.themes.Soft(),\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
